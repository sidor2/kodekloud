## Event-Driven Processing with Amazon S3 and Lambda

1. Create a public S3 bucket named devops-public-10365. Ensure that the bucket allows public access to its objects.
2. Create a private S3 bucket named devops-private-11160. Ensure that the bucket does not allow public access.

5. Create a DynamoDB table named devops-S3CopyLogs with a partition key LogID (string). This table will store logs generated by the Lambda function, including details such as source bucket name, destination bucket name, and object key.

3. Create a Lambda function named devops-copyfunction. This function should be triggered by uploads to the public S3 bucket and should copy the uploaded file to the private bucket.
    - Create the necessary policies and a role named lambda_execution_role. Attach these policies to the role, and then link this role to the Lambda function.

4. lambda-function.py is already present under the /root/ directory on AWS client host, replace REPLACE-WITH-YOUR-DYNAMODB-TABLE and REPLACE-WITH-YOUR-PRIVATE-BUCKET values.


- For testing upload the file sample.zip located in the /root directory on the client host to the public S3 bucket. The Lambda function should trigger and copy the file to the private bucket.
- Verify that the file has been successfully copied to the private bucket by checking the private bucket in the S3 console.
- Verify that a log entry has been created in the DynamoDB table containing the file copy details.